{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa5f085",
   "metadata": {},
   "source": [
    "## En esta parte agrego delta scutis que salieron hace muy poco! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac84f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ident = pd.read_csv(\"ident.dat\",delim_whitespace=True,names=[\"ID\",\"Subtype\",\"RA\",\"DEC\",\"field\",\"other\",\"a\",\"b\",\"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "star_info = [\n",
    "    \"ID\",\n",
    "    \"Intensity mean I-band magnitude\",\n",
    "    \"Intensity mean V-band magnitude\",\n",
    "    \"per\",\n",
    "    \"Uncertainty of primary period\",\n",
    "    \"Time of maximum brightness (HJD-2450000)\",\n",
    "    \"I-band amplitude (maximum-minimum)\",\n",
    "    \"Fourier coefficient R_21\",\n",
    "    \"Fourier coefficient phi_21\",\n",
    "    \"Fourier coefficient R_31\",\n",
    "    \"Fourier coefficient phi_31\",\n",
    "    \"Secondary period (if available)\",\n",
    "    \"Uncertainty of secondary period\",\n",
    "    \"aTime of maximum brightness (HJD-2450000)\",\n",
    "    \"aI-band amplitude (maximum-minimum)\",\n",
    "    \"Fourier coefficient R_21 (2)\",\n",
    "    \"Fourier coefficient phi_21 (2)\",\n",
    "    \"Fourier coefficient R_31 (2)\",\n",
    "    \"Fourier coefficient phi_31 (2)\",\n",
    "    \"Tertiary period (if available)\",\n",
    "    \"Uncertainty of tertiary period\",\n",
    "    \"Time of maximum brightness (HJD-2450000) (3)\",\n",
    "    \"I-band amplitude (maximum-minimum) (2)\",\n",
    "    \"Fourier coefficient R_21 (3)\",\n",
    "    \"Fourier coefficient phi_21 (3)\",\n",
    "    \"Fourier coefficient R_31 (3)\",\n",
    "    \"Fourier coefficient phi_31 (3)\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsct = pd.read_csv(\"dsct.dat\",delim_whitespace=True,names=star_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ident[\"types\"] = \"dsct\"\n",
    "ident[\"database\"] = 4\n",
    "ident[\"field\"] = \"smc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deae350",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsct = dsct[[\"ID\",\"per\"]].merge(ident[[\"ID\",\"Subtype\",\"RA\",\"DEC\",\"types\",\"database\",\"field\"]],how=\"inner\",on=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f2d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(\"catalogos/0_catalog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.concat([catalog,dsct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.to_csv(\"catalogos/0_catalog.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3fcbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones de bibliotecas estándar\n",
    "# Importaciones de bibliotecas de sistema\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "# Importaciones de bibliotecas de terceros\n",
    "import wget\n",
    "import scipy.signal\n",
    "import h5py\n",
    "import psutil\n",
    "import ray\n",
    "\n",
    "# Importaciones de TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    MaxPooling2D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
    "from keras import backend as K \n",
    "\n",
    "# Importaciones de sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Importaciones de pandas\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# Importaciones de matplotlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "%matplotlib inline\n",
    "\n",
    "# Importaciones de seaborn\n",
    "import seaborn as sns\n",
    "gyr = [\"#ffa600\",\n",
    "        '#003f5c',\n",
    "       \"#58508d\",\n",
    "       \"#ff6361\",\n",
    "       \"#ffd380\",\n",
    "       \"#bc5090\",\n",
    "       \"#129675\"\n",
    "      ]\n",
    "palet = sns.palplot(sns.color_palette(gyr))\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Importaciones de plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Importaciones de numpy\n",
    "import numpy as np\n",
    "\n",
    "# Importaciones de astropy\n",
    "from astropy.io import fits\n",
    "from astropy.timeseries import LombScargle\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "# Importaciones para el equilibrio de los datos\n",
    "from imblearn.keras import BalancedBatchGenerator\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "\n",
    "# Establecer la semilla para TensorFlow\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Obtén el número de CPUs\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "\n",
    "\n",
    "class BalancedDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates data for Keras Sequence based data generator. \n",
    "       Suitable for building data generator for training and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, batch_size=64):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_indices = [np.where(y == i)[0] for i in self.classes]\n",
    "        self.length = min([len(i) for i in self.class_indices]) // self.batch_size * len(self.classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        for class_index in self.class_indices:\n",
    "            i = idx % (len(class_index) // self.batch_size)\n",
    "            batch_x.append(self.x[class_index[i * self.batch_size:(i + 1) * self.batch_size]])\n",
    "            batch_y.append(self.y[class_index[i * self.batch_size:(i + 1) * self.batch_size]])\n",
    "        return np.concatenate(batch_x), np.concatenate(batch_y)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        for class_index in self.class_indices:\n",
    "            np.random.shuffle(class_index)\n",
    "\n",
    "\n",
    "# Funciones\n",
    "def descarga_wget(database,ID,path_3,path_4):\n",
    "    _,field,types,_ = ID.lower().split(\"-\")\n",
    "    try :\n",
    "        if types==\"ell\":\n",
    "            types=\"ecl\"\n",
    "        if database==4:\n",
    "            if ((field ==\"blg\") |(field ==\"gd\"))&((types ==\"ecl\")|(types ==\"lpv\")|(types ==\"dsct\")):\n",
    "                url = \"http://ftp.astrouw.edu.pl/ogle/ogle4/OCVS/\"+field+\"/\"+types+\"/phot_ogle4/I/\"+ ID +\".dat\"\n",
    "                wget.download(url,path_4)\n",
    "                return 1\n",
    "            else:\n",
    "                url = \"http://ftp.astrouw.edu.pl/ogle/ogle4/OCVS/\"+field+\"/\"+types+\"/phot/I/\"+ ID +\".dat\"\n",
    "                wget.download(url,path_4)\n",
    "                return 1\n",
    "                \n",
    "        if database==3:\n",
    "            url = \"http://ftp.astrouw.edu.pl/ogle/ogle3/OIII-CVS/\" +field+\"/\"+types+\"/phot/I/\"+ ID +\".dat\"\n",
    "            wget.download(url,path_3)\n",
    "            return 1\n",
    "    except:\n",
    "        return 0\n",
    "            \n",
    "@ray.remote\n",
    "def review_open_data(nomb,path_datos,database):\n",
    "        path= path_datos[database]\n",
    "        try :\n",
    "            df = pd.read_csv(f\"{path}/{nomb}.dat\",delim_whitespace=True,names=[\"jd\",\"mag\",\"err\"])\n",
    "            df_sigma = df.loc[(df[\"mag\"] < np.mean(df[\"mag\"]) + 3*np.std(df[\"mag\"])) & ( df[\"mag\"] > np.mean(df[\"mag\"]) - 3*np.std(df[\"mag\"]) )]\n",
    "            obs_eliminadas = len(df) - len(df_sigma)\n",
    "            amplitud = df_sigma[\"mag\"].max() - df_sigma[\"mag\"].min()\n",
    "            mag_mean = df_sigma[\"mag\"].mean()\n",
    "            mag_std = df_sigma[\"mag\"].std()\n",
    "            err_mean = df_sigma[\"err\"].mean()\n",
    "            err_std = df_sigma[\"err\"].std()\n",
    "            obs_final = len(df_sigma)\n",
    "            obs_inicial = len(df)\n",
    "            return 1,nomb,database,obs_eliminadas,amplitud,mag_mean,mag_std,err_mean,err_std,obs_final,obs_inicial\n",
    "        except:\n",
    "            return 0,nomb,database,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan\n",
    "\n",
    "        \n",
    "def ra_dec_to_degrees(ra_str, dec_str):\n",
    "    # Convertir las coordenadas RA y DEC en objetos SkyCoord\n",
    "    coord = SkyCoord(ra=ra_str, dec=dec_str, unit=(u.hourangle, u.deg))\n",
    "\n",
    "    # Obtener las coordenadas en grados\n",
    "    ra_deg = coord.ra.degree\n",
    "    dec_deg = coord.dec.degree\n",
    "\n",
    "    return ra_deg, dec_deg\n",
    "\n",
    "def fase_datos(path_datos,database,nomb,per_vsx):\n",
    "    path= path_datos[database]\n",
    "    df = pd.read_csv(f\"{path}/{nomb}.dat\",delim_whitespace=True,names=[\"jd\",\"mag\",\"err\"])\n",
    "    df_sigma = df.loc[(df[\"mag\"] < np.mean(df[\"mag\"]) + 3*np.std(df[\"mag\"])) & ( df[\"mag\"] > np.mean(df[\"mag\"]) - 3*np.std(df[\"mag\"]) )]\n",
    "    if len(df_sigma)>2000:\n",
    "        df_sigma = df_sigma.sample(2000,random_state=42).reset_index(drop=True)\n",
    "    fase_vsx = np.mod(df_sigma.jd, per_vsx) / per_vsx\n",
    "    mag_vsx,t_vsx,err_vsx =df_sigma.mag,df_sigma.jd,df_sigma.err\n",
    "    return fase_vsx,mag_vsx,t_vsx\n",
    "\n",
    "def make_2d_histogram(n_bins_x,n_bins_y,data_mag,data_fase,norm_max):\n",
    "    bins_x = np.linspace(0,1, n_bins_x) # Curves in phase between 0 and 2.\n",
    "    bins_y = np.linspace( data_mag.min(), data_mag.max(), n_bins_y)\n",
    "    hist_data, _xbins, _ybins = np.histogram2d(data_fase, data_mag, bins=(bins_x, bins_y))\n",
    "    # Data in histogram is transposed, then transpose it just once:\n",
    "    if norm_max==\"max\":\n",
    "        norm_max = hist_data.max()\n",
    "        hist_data_norm = hist_data / norm_max\n",
    "        hist_data_transposed = hist_data_norm.transpose()\n",
    "        hdu = fits.PrimaryHDU(data=hist_data_transposed)\n",
    "        return hdu\n",
    "    else:\n",
    "        norm_max = float(norm_max)\n",
    "        hist_data[hist_data > norm_max ] = norm_max\n",
    "        hist_data_norm = hist_data / norm_max\n",
    "        hist_data_transposed = hist_data_norm.transpose()\n",
    "        hdu = fits.PrimaryHDU(data=hist_data_transposed)\n",
    "        return hdu\n",
    "\n",
    "def split_random(df,numero_dividir,col_name):\n",
    "    for types in df[\"types\"].unique():\n",
    "        df_var = df.loc[df[\"types\"]==types].sample(numero_dividir,random_state=42)\n",
    "        df_train,df_test = train_test_split(df_var,random_state=42,test_size=0.13)\n",
    "        df_train,df_val = train_test_split(df_train,random_state=42,test_size=0.15)\n",
    "        df.loc[df_train.index,col_name] = \"train\"\n",
    "        df.loc[df_val.index,col_name] = \"val\"\n",
    "        df.loc[df_test.index,col_name] = \"test\"\n",
    "    return df\n",
    "    \n",
    "\n",
    "def split_data_balanced(df,numero_dividir):\n",
    "    df[\"combined\"] = list(zip(df[\"obs_final\"],\n",
    "                          df[\"amplitud\"],\n",
    "                          df[\"mag_mean\"],\n",
    "                          df[\"mag_std\"],\n",
    "                          df[\"field\"],\n",
    "                         df[\"err_mean\"],\n",
    "                         df[\"per\"],\n",
    "                         df[\"err_std\"]))\n",
    "    combined_weight = df['combined'].value_counts(normalize=True)\n",
    "    df['combined_weight'] = df['combined'].apply(lambda x: combined_weight[x])\n",
    "    subsample = df.sample(numero_dividir, weights=df['combined_weight'])\n",
    "    for types in df[\"types\"].unique():\n",
    "        df_var = df.loc[df[\"types\"]==types].sample(numero_dividir,\n",
    "                                                             weights=df['combined_weight'],\n",
    "                                                             random_state=42)\n",
    "        df_train = df_var.sample(frac=0.8,\n",
    "                         weights=df['combined_weight'],\n",
    "                         random_state=42)\n",
    "        df_var = df_var.drop(df_train.index)\n",
    "        \n",
    "        df_val = df_var.sample(frac=0.5,\n",
    "                         weights=df['combined_weight'],\n",
    "                         random_state=42)\n",
    "        \n",
    "        df_test = df_var.drop(df_val.index)\n",
    "        \n",
    "        df.loc[df_train.index,\"entrenamiento_8mil_balanced\"] = \"train\"\n",
    "        df.loc[df_val.index,\"entrenamiento_8mil_balanced\"] = \"val\"\n",
    "        df.loc[df_test.index,\"entrenamiento_8mil_balanced\"] = \"test\"\n",
    "            \n",
    "        \n",
    "    return df\n",
    "\n",
    "def plot_obs_dist(df,split_name):\n",
    "    sns.set_context(\"paper\")\n",
    "    gyr = [\"#890B96\",'#FFCF3D',\"#129675\"]\n",
    "    sns.set_palette(gyr)\n",
    "\n",
    "    columns = [\"obs_final\", \"amplitud\", \"mag_mean\", \"field\", \"err_mean\", \"per\", \"mag_std\", \"err_std\"]\n",
    "    labels = [r'$n_{obs}$', r'$Amplitude$', 'Mean Magnitude', \"Field\", 'Mean Error', 'Period', \n",
    "              'Magnitude standard deviations', 'Magnitude Error standard deviations']\n",
    "    log_scales = [True, True, False, False, True, True, True, False]\n",
    "    x_ticks = [[10**2,10**3,10**4], [10**-1,1,10**1], None, None, [10**-2,10**-1,10**0], [10**-1,10**1,10**3],\n",
    "               None, [0,0.3,0.6]]\n",
    "    y_scale_log = [False, False, False, False, False, False, False, True]\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(7,15))\n",
    "\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        sns.histplot(ax=ax, data=df, x=columns[i], hue=split_name, bins=30,\n",
    "                     stat=\"density\", log_scale=log_scales[i], fill=True, common_norm=False)\n",
    "        ax.set(xlabel=labels[i], ylabel=\"\")\n",
    "        if x_ticks[i] is not None:\n",
    "            ax.set_xticks(x_ticks[i])\n",
    "        if y_scale_log[i]:\n",
    "            ax.set_yscale(\"log\")\n",
    "        if i != 0: # remove legend for all but the first subplot\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "    plt.rc('xtick', labelsize=13) \n",
    "    plt.rc('ytick', labelsize=13)\n",
    "    fig.tight_layout()\n",
    "    fig.text(-0.01,0.5,\"Density\", size=13, rotation=90)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "\n",
    "def plot_histograms(df, estrellas, path_datos,norm):\n",
    "    estrellas_plot = df.loc[df[\"ID\"].isin(estrellas)].reset_index(drop=True)\n",
    "\n",
    "    fig, ax = plt.subplots(len(estrellas), 2, figsize=(5,10), sharex=\"col\")\n",
    "    for i in range(len(estrellas_plot)):\n",
    "        fase, mag, t_vsx = fase_datos(path_datos, estrellas_plot[\"database\"][i], estrellas_plot[\"ID\"][i], estrellas_plot[\"per\"][i])\n",
    "\n",
    "        ax[i,0].set_ylim(mag.max(), mag.min())\n",
    "        ax[i,0].set_yticks(np.linspace(mag.min() + (mag.max() - mag.min()) /10, mag.max() - (mag.max() - mag.min()) /10, 4))\n",
    "        ax[i,0].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        sns.scatterplot(x=fase, y=mag, c=t_vsx, s=5, ax=ax[i,0])\n",
    "\n",
    "        hdu = make_2d_histogram(32+1, 32+1, mag, fase, norm_max=norm)\n",
    "        ax[i,1].imshow(hdu.data, interpolation='nearest', aspect='auto')\n",
    "        ax[i,1].set_yticklabels([])\n",
    "        ax[i,1].set_xticklabels([])\n",
    "\n",
    "    fig.text(0.5, 0, \"Phase\", size=13)\n",
    "    fig.text(-0.01, 0.5, \"I Mag\", size=13, rotation=90)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "    plt.savefig(\"hist_2d_fase.pdf\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    return hdu \n",
    "\n",
    "@ray.remote\n",
    "def make_lc_hist(nomb,\n",
    "                 per_vsx,\n",
    "                 path_datos,\n",
    "                 database,\n",
    "                 aug,\n",
    "                 rng,\n",
    "                 g,\n",
    "                 bins,\n",
    "                N):\n",
    "        path= path_datos[database]\n",
    "        df = pd.read_csv(f\"{path}/{nomb}.dat\",delim_whitespace=True,names=[\"d\",\"mag\",\"e\"])\n",
    "        df_sigma = df.loc[(df[\"mag\"] < np.mean(df[\"mag\"]) + 3*np.std(df[\"mag\"])) & ( df[\"mag\"] > np.mean(df[\"mag\"]) - 3*np.std(df[\"mag\"]) )].reset_index(drop=True)\n",
    "        if int(aug) == 0:\n",
    "            df_sigma[\"fase\"] = np.mod(df_sigma.d, per_vsx) / per_vsx\n",
    "            if len(df_sigma)>2000:\n",
    "                df_sigma = df_sigma.sample(2000,random_state=42)\n",
    "            hdu =make_2d_histogram(32+1,32+1,df_sigma.mag,df_sigma.fase, norm_max=7)\n",
    "            return hdu.data\n",
    "        \n",
    "        if int(aug) == 1:\n",
    "            df_sigma[\"fase\"] =  np.mod(df_sigma.d - g, per_vsx) / per_vsx\n",
    "            df_sigma[\"mag\"] = df_sigma[\"mag\"] + rng.normal(0, df_sigma[\"e\"],len(df_sigma))\n",
    "            df_sigma[\"fase_bin\"] = pd.cut(df_sigma[\"fase\"],bins=int(bins))\n",
    "            df_bins = pd.DataFrame(df_sigma.groupby(\"fase_bin\")[\"fase\"].mean())\n",
    "            df_bins[\"mag\"] = df_sigma.groupby(\"fase_bin\")[\"mag\"].mean()\n",
    "            df_bins[\"e\"] = df_sigma.groupby(\"fase_bin\")[\"e\"].mean()\n",
    "            df_bins[\"d\"] = df_sigma.groupby(\"fase_bin\")[\"d\"].mean()\n",
    "            hdu =make_2d_histogram(32+1,32+1,df_bins.mag,df_bins.fase,norm_max=N)\n",
    "            return hdu.data\n",
    "\n",
    "def create_hdf5(df,path_datos,rng):\n",
    "    results_ids = []\n",
    "    for i in range(len(df)):\n",
    "        hdu = make_lc_hist.remote(df[\"ID\"][i],\n",
    "                                  df[\"per\"][i],\n",
    "                                  path_datos,\n",
    "                                  df[\"database\"][i],\n",
    "                                  df[\"aug\"][i],\n",
    "                                  rng,\n",
    "                                  df[\"g\"][i],\n",
    "                                  df[\"bins\"][i],\n",
    "                                 df[\"N\"][i])\n",
    "        results_ids.append((hdu))\n",
    "    x = np.empty((len(df), 32, 32))\n",
    "    for i,key in enumerate(results_ids):\n",
    "        ima = ray.get(key)\n",
    "        x[i] = ima\n",
    "    x = np.expand_dims(x, axis=3)\n",
    "    return x\n",
    "\n",
    "def make_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), input_shape=(32, 32, 1),activation=\"relu\",padding=\"same\"),\n",
    "    tf.keras.layers.Conv2D(16, (3,3),activation=\"relu\",padding=\"same\"),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3),activation=\"relu\",padding=\"same\"),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3),activation=\"relu\",padding=\"same\"),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(1024,activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(512,activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "   # tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    tf.keras.layers.Dense(8, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-4,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=0.1), loss=\"sparse_categorical_crossentropy\", metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def train_models(df_lista, keys_lista, data, prueba_8mil,epochs=200, use_balanced_generator=False):\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    validation_datagen = ImageDataGenerator()\n",
    "    idx_val = prueba_8mil.loc[prueba_8mil['prueba_13080mil']==\"val\"].index.values\n",
    "    val_label = data['prueba_13080mil_label'][idx_val]\n",
    "    val_data = data[\"prueba_13080mil\"][idx_val]\n",
    "    val_gen = validation_datagen.flow(val_data, val_label, batch_size=32, shuffle=True)\n",
    "\n",
    "    for df, test_name in zip(df_lista, keys_lista):\n",
    "        K.clear_session()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n",
    "\n",
    "        model_history_log_file = f\"history_softmax_{'batchBalanced_' if use_balanced_generator else ''}{test_name}.csv\"\n",
    "        csv_logger = CSVLogger(model_history_log_file, append=False)\n",
    "\n",
    "        checkpoint_path = f\"training_softmax_{'batchBalanced_' if use_balanced_generator else ''}{test_name}/cp.ckpt\"\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, save_best_only=False, verbose=1)\n",
    "\n",
    "        callbacks = [csv_logger, cp_callback, early_stopping]\n",
    "        \n",
    "        if use_balanced_generator:\n",
    "            idx_train = df.loc[(df[test_name]!=\"test\")&(df[test_name]!=\"val\")&(df[\"aug\"]==0)].index.values\n",
    "        else :\n",
    "            idx_train = df.loc[(df[test_name]!=\"test\")&(df[test_name]!=\"val\")].index.values\n",
    "\n",
    "        bz = int((len(idx_train) * 64)/ len(prueba_8mil.loc[prueba_8mil['prueba_13080mil']==\"train\"]))\n",
    "    \n",
    "        train_label = data[f'{test_name}_label'][idx_train]\n",
    "        \n",
    "        train_data = data[test_name][idx_train]\n",
    "        \n",
    "        if use_balanced_generator:\n",
    "            train_gen = BalancedDataGenerator(train_data, train_label, batch_size=64)\n",
    "        else:\n",
    "            train_datagen = ImageDataGenerator()\n",
    "            train_gen = train_datagen.flow(train_data, train_label, batch_size=bz, shuffle=True)\n",
    "\n",
    "        model = make_model()\n",
    "        print(f\"Use balanced Generator [{use_balanced_generator}] \\n Data: {len(train_data)} \\n -----------------------------------------------------------------------------------\")\n",
    "        history = model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)\n",
    "    return\n",
    "\n",
    "    \n",
    "def augmented_to(ID,count,df):\n",
    "    a = (np.linspace(0,9,count)*100).astype(int)\n",
    "    np.sort(rng.uniform(low=0 + 1/32, high=1 - 1/32, size=1000))[a]\n",
    "    df.loc[df[\"ID\"]==ID,\"g\"] = np.sort(rng.uniform(low=0 + 1/32, high=1 - 1/32, size=1000))[a]\n",
    "    return\n",
    "\n",
    "def plot_accuracy_and_loss(path,file_names, title_names, amarillo_train, purpura_val, output_file=\"training_.pdf\"):\n",
    "    plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "    sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 1.5})\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    fig, axs = plt.subplots(2, len(file_names), sharex=\"col\", sharey=\"row\")\n",
    "    metrics = [\"acc\", \"val_acc\", \"loss\", \"val_loss\"]\n",
    "    labels = ['Training data Augmentation', 'Validation data Augmentation', 'Training data Augmentation', 'Validation data Augmentation']\n",
    "    labels_batch = ['Training batch balanced', 'Validation batch balanced', 'Training batch balanced', 'Validation batch balanced']    \n",
    "    colors = [amarillo_train, purpura_val, amarillo_train, purpura_val]\n",
    "    linestyles = [\"-.\", \".as\", \"dashdot\", \"dashdot\"]\n",
    "\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        batch_name = file_name.split(\"softmax\")[0]+\"softmax_batchBalanced\"+file_name.split(\"softmax\")[1]\n",
    "        df = pd.read_csv(f\"{path}/{file_name}.csv\")\n",
    "        df_batch = pd.read_csv(f\"{path}/{batch_name}.csv\")\n",
    "\n",
    "        for j, metric in enumerate(metrics):\n",
    "            sns.lineplot(ax=axs[j//2, i], data=df, x=\"epoch\", y=metric, \n",
    "                         color=colors[j], label=labels[j], linestyle=\"solid\")\n",
    "            if i > 0:\n",
    "                sns.lineplot(ax=axs[j//2, i], data=df_batch, x=\"epoch\", y=metric, \n",
    "                             color=colors[j], label=labels_batch[j], linestyle=\"dashed\")\n",
    "\n",
    "        axs[0,i].set_title(title_names[i])\n",
    "        axs[0,i].set_ylim([0.6,1])\n",
    "        axs[1,i].set_ylim([0.6,1])\n",
    "        axs[0,i].set_yticks(np.linspace(0.6,1,8))\n",
    "        axs[1,i].set_yticks(np.linspace(0,0.9,8))\n",
    "        axs[0,i].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        axs[1,i].yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        axs[1,i].set_xlabel('Epoch')\n",
    "\n",
    "        axs[0,i].get_legend().remove()\n",
    "        axs[1,i].get_legend().remove()\n",
    "\n",
    "    axs[0,0].set_ylabel('Accuracy')\n",
    "    axs[1,0].set_ylabel('Loss')\n",
    "\n",
    "    handles, labels = axs[1,2].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc=(0.19,0.5), ncol=4, fancybox=True, shadow=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.05)\n",
    "    plt.savefig(output_file, bbox_inches=\"tight\")\n",
    "    return\n",
    "def run_analysis(tests,titles,entrenamiento,filename=\"../data_Paper_OGLE/Data_08Sep.hdf5\",\n",
    "                 csv_file=\"../data_Paper_OGLE/catalogos/prueba_8mil.csv\"):\n",
    "    # Load data\n",
    "    data = h5py.File(filename, 'r+')\n",
    "    df_8mil = pd.read_csv(csv_file)\n",
    "    idx_test = df_8mil.loc[df_8mil[\"prueba_8mil\"]==\"test\"].index.values\n",
    "    test = df_8mil.loc[df_8mil[\"prueba_8mil\"]==\"test\"]\n",
    "    test = test.drop(columns={\"prueba_8mil\",\"aug\",\"g\",\"bins\",\"GroupID\",\"GroupSize\"})\n",
    "    \n",
    "    # Prepare data generator\n",
    "    test_datagen = ImageDataGenerator()\n",
    "    test_gen = test_datagen.flow(\n",
    "        data[\"prueba_13080mil\"][idx_test],\n",
    "        data[\"prueba_13080mil_label\"][idx_test],\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    sns.set_context(\"paper\",font_scale=3)\n",
    "    model = make_model()\n",
    "\n",
    "    num_tests = len(tests)\n",
    "    rows = 2  # Ahora queremos 2 filas\n",
    "    cols = 3  # Y 3 columnas\n",
    "\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(11*3, 10*2), sharey=\"row\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.2, right=0.7)\n",
    "\n",
    "    # Aplanar el array de ejes para iterar fácilmente\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, prueba in enumerate(tests):\n",
    "        model.load_weights(f\"{entrenamiento}/{prueba}/cp.ckpt\")\n",
    "        prediction(test, test_gen, model, prueba)\n",
    "\n",
    "        # Calculate F1 Score\n",
    "        f1 = f1_score(test_gen.y, test[f\"label_predict_{prueba}\"], average='weighted')\n",
    "\n",
    "        # Plots\n",
    "        array, annot = C_M(test_gen.y, test[f\"label_predict_{prueba}\"])\n",
    "        sns.heatmap(array, annot=annot, fmt='', vmin=0, vmax=np.sum(array, axis=1)[0], cmap=\"BuPu\",\n",
    "                    annot_kws={\"fontsize\":25}, linewidth=1, ax=ax[i], cbar=False)\n",
    "        ax[i].set_yticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5])\n",
    "        ax[i].set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5])\n",
    "        ax[i].set_yticklabels(['ELL', 'Mira', 'CEP', 'DST', 'ECL', 'LPV', 'RRL', \"Rndm\"])\n",
    "        ax[i].set_xticklabels(['ELL', 'Mira', 'CEP', 'DST', 'ECL', 'LPV', 'RRL', \"Rndm\"], rotation=45)\n",
    "        prueba = prueba.split(\"prueba\")[1]\n",
    "        # Add title and F1 Score\n",
    "        ax[i].set_title(f'{titles[i]}\\nF1 Score: {f1:.3f}')\n",
    "\n",
    "    # Eliminar el último subplot si el número de tests no llena todos los subplots\n",
    "    if num_tests < rows * cols:\n",
    "        fig.delaxes(ax[-1])\n",
    "\n",
    "    # Etiqueta general para el eje y\n",
    "    fig.text(-0.02, 0.5, 'True Label', va='center', rotation='vertical', fontsize=30)\n",
    "\n",
    "    # Etiqueta general para el eje x\n",
    "    fig.text(0.5, -0.02, 'Predicted Label', ha='center', fontsize=30)\n",
    "\n",
    "    fig.tight_layout(pad=0)\n",
    "    plt.savefig(\"CM.pdf\", bbox_inches=\"tight\")\n",
    "    return test\n",
    "\n",
    "def prediction(df,image_gen,model,prueba):\n",
    "    %matplotlib inline\n",
    "    grupos = ['ELL', 'Mira', 'cep', 'dsct', 'ecl', 'lpv', 'rrlyr',\"random\"]\n",
    "    label_predict = []\n",
    "    porcentaje_predict = []\n",
    "    nombres = []\n",
    "    for i in model.predict(image_gen.x):\n",
    "        idx = np.argmax(i)\n",
    "        label_predict.append(np.argmax(i))\n",
    "        porcentaje_predict.append(i[idx])\n",
    "        nombres.append(grupos[np.argmax(i)])\n",
    "    df[f\"label_predict_{prueba}\"] = label_predict\n",
    "    df[f\"porcentaje_predict_{prueba}\"] = porcentaje_predict\n",
    "    df[f\"nombres_predict_{prueba}\"] = nombres\n",
    "    return\n",
    "\n",
    "def C_M(label,predict_label):\n",
    "    array = np.array(tf.math.confusion_matrix(label,predict_label) )\n",
    "    df = pd.DataFrame(array)\n",
    "    perc = df.copy()\n",
    "    cols=perc.columns.values\n",
    "    perc[cols]=perc[cols].div(perc[cols].sum(axis=1), axis=0).multiply(100)\n",
    "    annot=df.round(2).astype(str) + \"\\n\" + perc.round(1).astype(str) + \"%\"\n",
    "    return array,annot\n",
    "\n",
    "def metricas(labels,predict):\n",
    "    print(\"Accuracy:\", \"%0.2f\" % metrics.accuracy_score(labels,predict))\n",
    "    print(\"macro precision: \",\"%0.2f\" %  metrics.precision_score(labels,predict, average='macro'))\n",
    "    print(\"macro recall: \",\"%0.2f\" %  metrics.recall_score(labels,predict, average='macro'))\n",
    "    print(\"macro F1: \",\"%0.2f\" %  metrics.f1_score(labels,predict, average='macro'))\n",
    "    print(metrics.classification_report(labels,predict, digits=2))\n",
    "    report = metrics.classification_report(labels,predict, output_dict=True, digits=2)\n",
    "    return report\n",
    "\n",
    "def train_random_forest(X_train, y_train, X_test, y_test, n_estimators=500, random_state=42):\n",
    "    # Crea el clasificador Random Forest\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    \n",
    "    # Entrena el clasificador\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predice las clases para el conjunto de test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Crea una figura y ejes para la trama\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    array, annot = C_M(y_test, y_pred)\n",
    "    sns.heatmap(array, annot=annot, fmt='', vmin=0, vmax=np.sum(array, axis=1)[0], cmap=\"BuPu\",\n",
    "                annot_kws={\"fontsize\":15}, linewidth=1, ax=ax, cbar=False)\n",
    "    \n",
    "    ax.set_yticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5])\n",
    "    ax.set_yticklabels(['ELL', 'Mira', 'Cep', 'Dsct', 'Ecl', 'Lpv', 'RRlyr'], fontsize=20)\n",
    "    ax.set_ylabel('True Label', fontsize=20)\n",
    "    \n",
    "    ax.set_xticks([0.5,1.5,2.5,3.5,4.5,5.5,6.5])\n",
    "    ax.set_xticklabels(['ELL', 'Mira', 'Cep', 'Dsct', 'Ecl', 'Lpv', 'RRlyr'], fontsize=20)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=20)\n",
    "    \n",
    "    # Add title and F1 Score\n",
    "    # Calculate F1 Score\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    ax.set_title(f'Random Forest\\nF1 Score: {f1:.2f}', fontsize=20)\n",
    "\n",
    "    fig.tight_layout(pad=0)\n",
    "    plt.savefig(\"CNN_And_RF.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "    return clf, y_pred\n",
    "\n",
    "datos = \"/dataworkspace/datos_ogle/datos\"\n",
    "\n",
    "path_datos_4 = datos + \"/datos_ogle_4/I\"\n",
    "\n",
    "path_datos_3 = datos + \"/datos_ogle_3/I\"\n",
    "path_datos = [\"_\",\"_\",\"_\",path_datos_3,path_datos_4]\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4b8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbba9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(\"catalogos/0_catalog.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c95919",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results_ids = []\n",
    "for i in range(len(catalog)):\n",
    "    results_ids.append(review_open_data.remote(catalog[\"ID\"][i],path_datos,catalog[\"database\"][i]))                                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d9ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.DataFrame(ray.get(results_ids),columns=[\"error\",\n",
    "                                            \"ID\", \"database\",\n",
    "                                        \"obs_eliminadas\",\n",
    "                                        \"amplitud\",\n",
    "                                        \"mag_mean\",\n",
    "                                        \"mag_std\",\n",
    "                                        \"err_mean\",\n",
    "                                        \"err_std\",\n",
    "                                        \"obs_final\",\n",
    "                                        \"obs_inicial\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c89fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog[['ID', 'RA', 'DEC', 'types', 'database', 'field', 'Subtype', 'per']].merge(df,how=\"inner\",on=[\"ID\",\"database\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = ra_dec_to_degrees(catalog[\"RA\"],catalog[\"DEC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f30d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog[\"ra_deg\"] = a\n",
    "catalog[\"dec_deg\"] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427188a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.to_csv(\"/home/variablestars/Nico/ogle_06_03_23/catalogos/0_catalog_descriptivo.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f225e5a",
   "metadata": {},
   "source": [
    "## Revisar Descarga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ee1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(\"catalogos/0_catalog_descriptivo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_descarga = catalog.loc[catalog[\"error\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a55b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descarga = catalog.drop_duplicates(subset=[\"types\",\"field\",\"database\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a6deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_descarga[\"error\"] = test_descarga.apply(lambda row: descarga_wget(row[\"database\"],\n",
    "                                                                       row[\"ID\"],\n",
    "                                                                      path_datos_3,\n",
    "                                                                      path_datos_4),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a5414",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "error_descarga[\"descarga_wget\"] = error_descarga.apply(lambda row: descarga_wget(row[\"database\"],\n",
    "                                                                       row[\"ID\"],\n",
    "                                                                      path_datos_3,\n",
    "                                                                      path_datos_4),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_descarga.loc[error_descarga[\"descarga_wget\"]!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb402da",
   "metadata": {},
   "source": [
    "## Distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefa31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(\"catalogos/0_catalog_descriptivo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = catalog.groupby(\"types\").count()[[\"ID\"]].rename(columns={\"ID\":\"Catalogo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537dbe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.loc[catalog[\"error\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"error download\"] = df[[\"Catalogo\"]].rename(columns={\"Catalogo\":\"ID\"}) - catalog.groupby(\"types\").count()[[\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.loc[(catalog[\"obs_final\"]!=0)] \n",
    "sns.histplot(data=catalog,x=\"obs_final\",bins=30,hue=\"types\"\n",
    "             ,palette=sns.color_palette(gyr),log_scale=True,\n",
    "    stat=\"density\", common_norm=False)\n",
    "plt.axvline(x=2000, color = 'r')\n",
    "plt.axvline(x=60, color = 'r')\n",
    "plt.savefig(\"paper_observaciones_final.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98939608",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Observaciones<60\"] = catalog.loc[(catalog[\"obs_final\"]<=60)|\n",
    "                                     (catalog[\"obs_final\"]==0)\n",
    "                                    ].groupby(\"types\").count()[[\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b77cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.loc[(catalog[\"obs_final\"]>60)] # aproximadamente 1 % de la mmuestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18435d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=catalog,x=catalog[\"obs_eliminadas\"] / catalog[\"obs_inicial\"],bins=50,hue=\"types\",palette=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cc02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.sort_values(by=\"database\",ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a8526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Duplicated ID\"] = catalog.loc[catalog.duplicated(subset=\"ID\")].groupby(\"types\").count()[[\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.drop_duplicates(subset=\"ID\",keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1efa168",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.groupby(\"types\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ade03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.to_csv(\"catalogos/0_catalog_descriptivo_antes_de_topcat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e53f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(\"catalogos/0_catalog_descriptivo_antes_de_topcat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea97c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_t = pd.read_csv(\"catalogos/0_catalog_descriptivo_despues_de_topcat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3cbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_t = catalog_t.loc[catalog_t[\"GroupID\"].isna()].drop(columns={\"col1\"}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17558697",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.groupby(\"types\").count()[[\"ID\"]] - catalog_t.groupby(\"types\").count()[[\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ecd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Topcat\"] = catalog.groupby(\"types\").count()[[\"ID\"]] - catalog_t.groupby(\"types\").count()[[\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"final\"] = catalog_t.groupby(\"types\").count()[[\"ID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca88523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Catalogo\"] - df[\"error download\"] - df[\"Observaciones<60\"] - df[\"Duplicated ID\"] - df[\"Topcat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89898fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.loc[catalog[\"per\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef8058",
   "metadata": {},
   "outputs": [],
   "source": [
    "cep_min = catalog.groupby(\"types\").count().sort_values(by=\"ID\",ascending=True)[\"ID\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce897f9",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006979d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.default_rng(42)\n",
    "datos = \"/media/variablestars/datos1/datos_ogle/datos\"\n",
    "path_datos_4 = datos + \"/datos_ogle_4/I\"\n",
    "\n",
    "path_datos_3 = datos + \"/datos_ogle_3/I\"\n",
    "path_datos = [\"_\",\"_\",\"_\",path_datos_3,path_datos_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pd.read_csv(\"catalogos/0_catalog_descriptivo_despues_de_topcat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ba2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = catalog.drop(columns={\"col1\"})\n",
    "catalog = catalog.loc[catalog[\"per\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82922a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cep_min = catalog.groupby(\"types\").count().sort_values(by=\"ID\",ascending=True)[\"ID\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec86e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_random = split_random(catalog,cep_min,\"entrenamiento_8mil_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_data_balanced = split_data_balanced(catalog,cep_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97633ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_data_balanced = df_split_data_balanced.loc[df_split_data_balanced[\"entrenamiento_8mil_balanced\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_split_random = df_split_random.loc[df_split_random[\"entrenamiento_8mil_randm\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ddff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_obs_dist(df_split_random,\"entrenamiento_8mil_randm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b49ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_obs_dist(df_split_data_balanced,\"entrenamiento_8mil_balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e257432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_8mil = df_split_random.loc[df_split_random[\"entrenamiento_8mil_balanced\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d549c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estrellas = ['OGLE-GD-RRLYR-04519', 'OGLE-BLG-LPV-240852', \"OGLE-SMC-ECL-2344\", \"OGLE-LMC-CEP-2430\", 'OGLE-LMC-LPV-83641', \"OGLE-BLG-DSCT-00245\", \"OGLE-BLG-ELL-007122\"]\n",
    "\n",
    "a = plot_histograms(prueba_8mil, estrellas, path_datos,10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f3c13e",
   "metadata": {},
   "source": [
    "## Augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc935ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogle = pd.read_csv(\"catalogos/0_catalog_descriptivo_despues_de_topcat.csv\")\n",
    "ogle = ogle.drop(columns={\"col1\"})\n",
    "ogle = ogle.loc[ogle[\"per\"]!=0]\n",
    "ogle = ogle[ogle[\"per\"].notna()].reset_index(drop=True)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(ogle.types)\n",
    "ogle[\"categorical_label\"] = le.transform(ogle.types)\n",
    "cep_min = ogle.groupby(\"types\").count().sort_values(by=\"ID\",ascending=True)[\"ID\"].values[0]\n",
    "ogle = split_random(ogle,cep_min,\"prueba_8mil\")\n",
    "prueba_8mil = ogle.dropna(subset=\"prueba_8mil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6559134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ogle = pd.read_csv(\"catalogos/0_catalog_descriptivo_despues_de_topcat.csv\")\n",
    "ogle = ogle.drop(columns={\"col1\"})\n",
    "ogle = ogle.loc[ogle[\"per\"]!=0]\n",
    "ogle = ogle[ogle[\"per\"].notna()].reset_index(drop=True)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(ogle.types)\n",
    "ogle[\"categorical_label\"] = le.transform(ogle.types)\n",
    "cep_min = ogle.groupby(\"types\").count().sort_values(by=\"ID\",ascending=True)[\"ID\"].values[0]\n",
    "ogle = split_random(ogle,cep_min,\"prueba_8mil\")\n",
    "prueba_8mil = ogle.dropna(subset=\"prueba_8mil\")\n",
    "\n",
    "ogle = ogle[~ogle[\"ID\"].isin(prueba_8mil[\"ID\"])].reset_index(drop=True)\n",
    "\n",
    "prueba_23_mil = pd.DataFrame()\n",
    "for types in ogle[\"types\"].unique():\n",
    "    df_types = ogle.loc[ogle[\"types\"]==types].reset_index(drop=True)\n",
    "    if len(df_types)> 15065:\n",
    "        df_types = df_types.sample(15065,random_state=42).reset_index(drop=True)\n",
    "    prueba_23_mil = pd.concat([prueba_23_mil,df_types])\n",
    "\n",
    "train_8mil = prueba_8mil.loc[prueba_8mil[\"prueba_8mil\"] == \"train\"].reset_index(drop=True)\n",
    "\n",
    "prueba_23_mil = pd.concat([prueba_23_mil,train_8mil])\n",
    "\n",
    "for types in prueba_23_mil[\"types\"].unique():\n",
    "    df_types = prueba_23_mil.loc[prueba_23_mil[\"types\"]==types].reset_index(drop=True)\n",
    "    resto = 24168 % len(df_types)\n",
    "    if 24168 // len(df_types) > 1:\n",
    "        repetir = 24168 // len(df_types) -1 \n",
    "        prueba_23_mil = pd.concat([prueba_23_mil,pd.concat([df_types]*repetir)])\n",
    "        prueba_23_mil = pd.concat([prueba_23_mil,df_types.sample(resto,random_state=42)])\n",
    "    if 24168 // len(df_types) == 1:\n",
    "        prueba_23_mil = pd.concat([prueba_23_mil,df_types.sample(resto,random_state=42)])\n",
    "    \n",
    "\n",
    "ogle = ogle[~ogle[\"ID\"].isin(prueba_23_mil[\"ID\"])].reset_index(drop=True)\n",
    "ogle.groupby(\"types\").count()\n",
    "\n",
    "prueba_60_mil = pd.DataFrame()\n",
    "for types in ogle[\"types\"].unique():\n",
    "    df_types = ogle.loc[ogle[\"types\"]==types].reset_index(drop=True)\n",
    "    if len(df_types)>= 36838:\n",
    "        df_types = df_types.sample(36838,random_state=42).reset_index(drop=True)\n",
    "    prueba_60_mil = pd.concat([prueba_60_mil,df_types])\n",
    "\n",
    "prueba_60_mil = pd.concat([prueba_60_mil,prueba_23_mil],axis=0)\n",
    "\n",
    "for types in prueba_60_mil[\"types\"].unique():\n",
    "    df_types = prueba_60_mil.loc[prueba_60_mil[\"types\"]==types].reset_index(drop=True)\n",
    "    resto = 61006 % len(df_types)\n",
    "    if 61006 // len(df_types) > 1:\n",
    "        repetir = 61006 // len(df_types) -1\n",
    "        prueba_60_mil = pd.concat([prueba_60_mil,pd.concat([df_types]*repetir)])\n",
    "        prueba_60_mil = pd.concat([prueba_60_mil,df_types.sample(resto,random_state=42)])\n",
    "    if 61006 // len(df_types) == 1:\n",
    "        prueba_60_mil = pd.concat([prueba_60_mil,df_types.sample(resto,random_state=42)])\n",
    "\n",
    "ogle = ogle[~ogle[\"ID\"].isin(prueba_60_mil[\"ID\"])].reset_index(drop=True)\n",
    "\n",
    "prueba_23_mil.loc[prueba_23_mil.duplicated(subset=\"ID\"),\"aug\"] = 1\n",
    "prueba_23_mil.loc[prueba_23_mil[\"aug\"].isna(),\"aug\"] = 0\n",
    "\n",
    "prueba_60_mil.loc[prueba_60_mil.duplicated(subset=\"ID\"),\"aug\"] = 1\n",
    "prueba_60_mil.loc[prueba_60_mil[\"aug\"].isna(),\"aug\"] = 0\n",
    "\n",
    "df_aux = prueba_23_mil.groupby(\"ID\").count()[[\"RA\"]].reset_index().rename(columns={\"RA\":\"count\"})\n",
    "prueba_23_mil = prueba_23_mil.merge(df_aux,how=\"left\",on=\"ID\")\n",
    "\n",
    "\n",
    "df_aux = prueba_60_mil.groupby(\"ID\").count()[[\"RA\"]].reset_index().rename(columns={\"RA\":\"count\"})\n",
    "prueba_60_mil = prueba_60_mil.merge(df_aux,how=\"left\",on=\"ID\")\n",
    "\n",
    "prueba_23_mil.loc[prueba_23_mil[\"count\"]>1].drop_duplicates(subset=\"ID\").parallel_apply(lambda row: augmented_to(row[\"ID\"],row[\"count\"],prueba_23_mil),axis=1)\n",
    "prueba_23_mil.loc[prueba_23_mil[\"aug\"]==1,\"bins\"] = prueba_23_mil.loc[prueba_23_mil[\"aug\"]==1].parallel_apply(lambda row: rng.choice(np.arange(int(row[\"obs_final\"]*0.5),\n",
    "                                           int(row[\"obs_final\"]*0.7))),axis=1)\n",
    "prueba_23_mil.loc[prueba_23_mil[\"bins\"]>2000,\"bins\"] = rng.choice(np.arange(1000,1500),len(prueba_23_mil.loc[prueba_23_mil[\"bins\"]>2000]))\n",
    "prueba_23_mil_sin_aug = prueba_23_mil.loc[prueba_23_mil[\"aug\"]==0].reset_index(drop=True)\n",
    "\n",
    "prueba_60_mil.loc[prueba_60_mil[\"count\"]>1].drop_duplicates(subset=\"ID\").parallel_apply(lambda row: augmented_to(row[\"ID\"],row[\"count\"],prueba_60_mil),axis=1)\n",
    "prueba_60_mil.loc[prueba_60_mil[\"aug\"]==1,\"bins\"] = prueba_60_mil.loc[prueba_60_mil[\"aug\"]==1].parallel_apply(lambda row: rng.choice(np.arange(int(row[\"obs_final\"]*0.5),\n",
    "                                           int(row[\"obs_final\"]*0.8))),axis=1)\n",
    "prueba_60_mil.loc[prueba_60_mil[\"bins\"]>2000,\"bins\"] = rng.choice(np.arange(1000,1500),len(prueba_60_mil.loc[prueba_60_mil[\"bins\"]>2000]))\n",
    "prueba_60_mil_sin_aug = prueba_60_mil.loc[prueba_60_mil[\"aug\"]==0].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_8mil[\"aug\"] = 0\n",
    "prueba_8mil[\"g\"] = np.nan\n",
    "prueba_8mil[\"bins\"] = np.nan\n",
    "prueba_8mil[\"N\"] = np.nan\n",
    "\n",
    "ogle[\"aug\"] = 0\n",
    "ogle[\"g\"] = np.nan\n",
    "ogle[\"bins\"] = np.nan\n",
    "ogle[\"N\"] = np.nan\n",
    "\n",
    "\n",
    "prueba_23_mil[\"g\"] = rng.uniform(low=0 + 1/32, high=1 - 1/32, size=len(prueba_23_mil))*prueba_23_mil[\"per\"]\n",
    "prueba_60_mil[\"g\"] = rng.uniform(low=0 + 1/32, high=1 - 1/32, size=len(prueba_60_mil))*prueba_60_mil[\"per\"]\n",
    "\n",
    "valores = [5, 7, 9, 11, 13, 15]\n",
    "\n",
    "prueba_23_mil[\"N\"] = np.random.choice(valores, len(prueba_23_mil)) \n",
    "prueba_60_mil[\"N\"] = np.random.choice(valores, len(prueba_60_mil)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b24c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_23_mil.loc[prueba_23_mil[\"prueba_8mil\"].isna(),\"prueba_8mil\"] = \"train\"\n",
    "prueba_60_mil.loc[prueba_60_mil[\"prueba_8mil\"].isna(),\"prueba_8mil\"] = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def randomize_per(group):\n",
    "    size = len(group)\n",
    "    low = group['per'].min()\n",
    "    high = group['per'].max()\n",
    "    group['per'] = np.random.uniform(low=low, high=high, size=size)\n",
    "    return group\n",
    "\n",
    "\n",
    "def make_random_period(df,n_star_split=\"auto\"):\n",
    "    if n_star_split==\"auto\":\n",
    "        for split in [\"train\",\"val\",\"test\"]:\n",
    "            df_var = df.loc[(df[\"prueba_8mil\"]==split)&(df[\"aug\"]==0)]\n",
    "            n_star_split = df_var.groupby(\"categorical_label\").count()[\"ID\"].max()\n",
    "            if np.isnan(n_star_split):\n",
    "                continue\n",
    "            if int(n_star_split) <= df_var.groupby(\"types\").count()[\"ID\"].min():\n",
    "                df_randoms = df_var.groupby(\"types\").sample(int(np.ceil(n_star_split/7)))\n",
    "                df_randoms = df_randoms.sample(n_star_split)\n",
    "            else:\n",
    "                df_randoms = df_var.groupby(\"types\").sample(int(df_var.groupby(\"types\").count()[\"ID\"].min()/7))\n",
    "                resto = df_var.drop(df_randoms.index, errors='ignore').sample(n_star_split - len(df_randoms))\n",
    "                df_randoms = pd.concat([df_randoms,resto])\n",
    "            df_randoms[\"categorical_label\"] = 7\n",
    "            df_randoms = df_randoms.groupby('types', group_keys=True).apply(randomize_per).reset_index(drop=True)\n",
    "            df_randoms[\"types\"] = df_randoms[\"types\"] +\"_\"+ \"random\"\n",
    "            df = pd.concat([df,df_randoms])\n",
    "    else:\n",
    "        for split in [\"train\"]:\n",
    "            df_var = df.loc[(df[\"prueba_8mil\"]==split)&(df[\"aug\"]==0)]\n",
    "            if np.isnan(n_star_split):\n",
    "                continue\n",
    "            if int(n_star_split) <= df_var.groupby(\"types\").count()[\"ID\"].min():\n",
    "                df_randoms = df_var.groupby(\"types\").sample(int(np.ceil(n_star_split/7)))\n",
    "                df_randoms = df_randoms.sample(n_star_split)\n",
    "            else:\n",
    "                df_randoms = df_var.groupby(\"types\").sample(int(df_var.groupby(\"types\").count()[\"ID\"].min()/7))\n",
    "                resto = df_var.drop(df_randoms.index, errors='ignore').sample(n_star_split - len(df_randoms))\n",
    "                df_randoms = pd.concat([df_randoms,resto])\n",
    "            df_randoms[\"categorical_label\"] = 7\n",
    "            df_randoms = df_randoms.groupby('types', group_keys=True).apply(randomize_per).reset_index(drop=True)\n",
    "            df_randoms[\"types\"] = df_randoms[\"types\"] +\"_\"+ \"random\"\n",
    "            df = pd.concat([df,df_randoms])\n",
    "\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4806ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_8mil_2 = make_random_period(prueba_8mil)\n",
    "prueba_23_mil_2 =  make_random_period(prueba_23_mil,24168)\n",
    "prueba_60_mil_2 =  make_random_period(prueba_60_mil,61006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_60_mil_2.to_csv(\"catalogos/prueba_60mil.csv\",index=False)\n",
    "prueba_23_mil_2.to_csv(\"catalogos/prueba_23mil.csv\",index=False)\n",
    "prueba_8mil_2.to_csv(\"catalogos/prueba_8mil.csv\",index=False)\n",
    "ogle.to_csv(\"catalogos/ogle_no_usado.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8149632",
   "metadata": {},
   "source": [
    "## Revisar aumento de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File(\"../data_Paper_OGLE/Data_08Sep.hdf5\", 'r+')\n",
    "prueba_60_mil = pd.read_csv(\"../data_Paper_OGLE/catalogos/prueba_60mil.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1686b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cep_to_plot = prueba_60_mil.loc[prueba_60_mil[\"types\"]==\"cep\"].head(10)\n",
    "row = cep_to_plot.drop_duplicates(subset=\"count\")[\"count\"].max()\n",
    "cep_to_plot_ID = cep_to_plot[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1,5, figsize=(30,8))\n",
    "fig.subplots_adjust(hspace=0, wspace=0)\n",
    "df_star = prueba_60_mil.loc[prueba_60_mil[\"ID\"] == cep_to_plot[\"ID\"].values[0]]\n",
    "df_star.loc[df_star[\"bins\"].isna(),\"N\"] = 7\n",
    "df_star.loc[df_star[\"bins\"].isna(),\"g\"] = 0\n",
    "df_star.loc[df_star[\"bins\"].isna(),\"bins\"] = df_star.loc[df_star[\"bins\"].isna()][\"obs_final\"]\n",
    "df_star = df_star.sample(5, random_state=4)\n",
    "df_star = df_star.sort_values(by=\"g\")\n",
    "\n",
    "for i, b in enumerate(df_star.index.values):\n",
    "    axes[i].set_yticks([])\n",
    "    axes[i].set_xticks([])\n",
    "    im = data['prueba_69721mil'][b]\n",
    "    n_obs = df_star[\"bins\"][b]\n",
    "    phi = df_star[\"g\"][b]\n",
    "    N = df_star[\"N\"][b]\n",
    "    axes[i].imshow(im, aspect='auto')\n",
    "    \n",
    "    # Definir tamaño de fuente\n",
    "    font_size = 13  # Puedes ajustar este valor según lo necesites\n",
    "    \n",
    "    # Agregar la información línea por línea en la esquina inferior izquierda\n",
    "    yOffset = 0.05\n",
    "    step = 0.06\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", edgecolor='black', facecolor='white', alpha=0.7)\n",
    "\n",
    "    axes[i].text(0.05, yOffset, r\"$N_{{\\mathrm{{Obs}}}}$: {}\".format(n_obs), transform=axes[i].transAxes, \n",
    "                 color='black', backgroundcolor='white', verticalalignment='bottom', bbox=bbox_props, fontsize=font_size)\n",
    "    yOffset += step\n",
    "    axes[i].text(0.05, yOffset, r\"$\\phi'$: {}\".format(round(phi,3)), transform=axes[i].transAxes, \n",
    "                 color='black', backgroundcolor='white', verticalalignment='bottom', bbox=bbox_props, fontsize=font_size)\n",
    "    yOffset += step\n",
    "    axes[i].text(0.05, yOffset, r\"$N$: {}\".format(N), transform=axes[i].transAxes, \n",
    "                 color='black', backgroundcolor='white', verticalalignment='bottom', bbox=bbox_props, fontsize=font_size)\n",
    "ogle_id = df_star[\"ID\"].unique()[0]\n",
    "# Agregar título\n",
    "fig.suptitle(f\"Data augmentation of {ogle_id}\", fontsize=25)\n",
    "\n",
    "# Guardar como PDF\n",
    "plt.savefig(\"nombre_del_archivo.pdf\", bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb440d8",
   "metadata": {},
   "source": [
    "### Hacer HDF5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_60_mil = pd.read_csv(\"catalogos/prueba_60mil.csv\")\n",
    "prueba_23_mil= pd.read_csv(\"catalogos/prueba_23mil.csv\")\n",
    "prueba_8mil = pd.read_csv(\"catalogos/prueba_8mil.csv\")\n",
    "ogle = pd.read_csv(\"catalogos/ogle_no_usado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total = h5py.File('Data_08Sep.hdf5', 'w')\n",
    "for test in [prueba_8mil,prueba_23_mil,prueba_60_mil,ogle]:\n",
    "    data_hist = create_hdf5(test,path_datos,rng)\n",
    "    name = int(len(test)/7)\n",
    "    data_total.create_dataset(f'prueba_{str(name)}mil_label', data=test['categorical_label'])\n",
    "    data_total.create_dataset(f'prueba_{str(name)}mil', data=data_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total = h5py.File('Data_08Sep.hdf5', 'r+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9a9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_total.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7185f",
   "metadata": {},
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_60_mil = pd.read_csv(\"catalogos/prueba_60mil.csv\")\n",
    "prueba_23_mil= pd.read_csv(\"catalogos/prueba_23mil.csv\")\n",
    "prueba_8mil = pd.read_csv(\"catalogos/prueba_8mil.csv\")\n",
    "ogle = pd.read_csv(\"catalogos/ogle_no_usado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = h5py.File(\"Data_08Sep.hdf5\", 'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba_8mil = prueba_8mil.rename(columns={\"prueba_8mil\":\"prueba_13080mil\"})\n",
    "prueba_23_mil = prueba_23_mil.rename(columns={\"prueba_8mil\":\"prueba_27620mil\"})\n",
    "prueba_60_mil = prueba_60_mil.rename(columns={\"prueba_8mil\":\"prueba_69721mil\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lista = [prueba_8mil,prueba_23_mil,prueba_60_mil]\n",
    "keys_lista = ['prueba_13080mil','prueba_27620mil','prueba_69721mil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd entrenamiento_8_sep/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aba693",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models(df_lista, keys_lista, data, prueba_8mil,epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd36232",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models(df_lista, keys_lista, data, prueba_8mil,epochs=600, use_balanced_generator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a18206",
   "metadata": {},
   "outputs": [],
   "source": [
    "amarillo_train = \"#FFCF3D\"\n",
    "purpura_val = \"#890B96\"\n",
    "file_names = [\"history_softmax_prueba_13080mil\",\n",
    "\"history_softmax_prueba_27620mil\",\n",
    "\"history_softmax_prueba_69721mil\"]\n",
    "title_names = [\n",
    "    'Train-9',\n",
    "    'Train-24',\n",
    "    'Train-60']\n",
    "plot_accuracy_and_loss(\"entrenamientos/entrenamiento_8_sep/\",file_names, title_names, amarillo_train, purpura_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b52dae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tests = ['training_softmax_prueba_13080mil','training_softmax_prueba_27620mil',\n",
    "         'training_softmax_batchBalanced_prueba_27620mil',\n",
    "         'training_softmax_prueba_69721mil',\n",
    "         'training_softmax_batchBalanced_prueba_69721mil']\n",
    "titles = ['Train-9 Undersampling','Train-24 Data Augmentation',\n",
    "         'Train-24 Batch Balanced',\n",
    "         'Train-60 Data Augmentation',\n",
    "         'Train-60 Batch Balanced']\n",
    "\n",
    "test = run_analysis(tests,titles,\"entrenamientos/entrenamiento_8_sep/\"\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = metricas(test[\"categorical_label\"],test[\"label_predict_training_softmax_batchBalanced_prueba_27620mil\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(df).T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76870cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[(test[\"categorical_label\"]!=7)&\n",
    "        (test[\"label_predict_training_softmax_batchBalanced_prueba_27620mil\"]==7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[\"prueba_13080mil\"][2220])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = h5py.File(\"../data_Paper_OGLE/Data_08Sep.hdf5\", 'r+')\n",
    "#ogle = pd.read_csv(\"catalogos/ogle_no_usado.csv\")\n",
    "df_8mil = pd.read_csv(\"../data_Paper_OGLE/catalogos/prueba_8mil.csv\")\n",
    "idx_test = df_8mil.loc[df_8mil[\"prueba_8mil\"]==\"test\"].index.values\n",
    "model_softmax = make_model()\n",
    "model_softmax.load_weights(\"entrenamientos/entrenamiento_8_sep/training_softmax_batchBalanced_prueba_69721mil/cp.ckpt\")\n",
    "df = pd.DataFrame(model_softmax.predict(data[\"prueba_13080mil\"][:]),columns=['ELL', 'Mira', 'cep', 'dsct', 'ecl', 'lpv', 'rrlyr',\"Random\"])\n",
    "df_8mil = pd.concat([df_8mil,df],axis=1)\n",
    "df_8mil = df_8mil.loc[df_8mil[\"categorical_label\"]!=7]\n",
    "X_train = df_8mil.loc[df_8mil[\"prueba_8mil\"]!=\"test\"][['ELL', 'Mira', 'cep', 'dsct', 'ecl', 'lpv', 'rrlyr',\"Random\",\n",
    "         \"per\",\"amplitud\",\"types\",\"categorical_label\"]]\n",
    "X_test = df_8mil.loc[df_8mil[\"prueba_8mil\"]==\"test\"][['ELL', 'Mira', 'cep', 'dsct', 'ecl', 'lpv', 'rrlyr',\"Random\",\n",
    "         \"per\",\"amplitud\",\"types\",\"categorical_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20f51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_random_forest(X_train.drop(columns={\"types\",\"categorical_label\"}),\n",
    "                   X_train[\"categorical_label\"],\n",
    "                   X_test.drop(columns={\"types\",\"categorical_label\"}),\n",
    "                   X_test[\"categorical_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcd2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
